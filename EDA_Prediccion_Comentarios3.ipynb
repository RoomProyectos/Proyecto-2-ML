{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfdde55",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "El procesamiento del lenguaje natural (**NLP**), se define ampliamente como la manipulación automática del lenguaje natural, como el habla y el texto, por parte del software.\n",
    "\n",
    "El estudio de **NLP** existe desde hace más de 50 años y surgió del campo de la lingüística con el auge de los ordenadores.\n",
    "\n",
    "**El lenguaje natural se refiere a la forma en que nosotros, los humanos, nos comunicamos entre nosotros.**\n",
    "\n",
    "Dada la importancia de este tipo de datos, debemos tener métodos para comprender y razonar sobre el lenguaje natural, tal como lo hacemos con otros tipos de datos.\n",
    "\n",
    "En Python, tenemos la librería **NLTK** (**Natural Language ToolKit**), éste es un modulo que contiene herramientas para el manejo del lenguaje natural.\n",
    "\n",
    "```python\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "_**Documentación:** https://www.nltk.org/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = pd.read_csv(\"Data/100178_Comentarios.csv\",sep=\";\")\n",
    "\n",
    "df_completo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f84472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_nan = df_completo.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_nan.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2dc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lista para almacenar el texto combinado de todas las filas\n",
    "texto = []\n",
    "texto_fila = []\n",
    "# Iteramos sobre cada fila del DataFrame\n",
    "for columns in df:\n",
    "    # Convertimos la fila (Series de pandas) a lista y luego la unimos en una cadena\n",
    "    texto_fila = (\" \".join(columns))\n",
    "\n",
    "# Unimos todas las filas combinadas en un texto final, separado por espacio\n",
    "texto_final = \" \".join(texto)\n",
    "\n",
    "print(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410879d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"texto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de379e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = []\n",
    "\n",
    "for columns in df[\"texto\"]:\n",
    "    df_lista = (columns)\n",
    "    texto.append(df_lista)\n",
    "\n",
    "texto_final = \" \".join(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenizar cada elemento de la lista df_str\n",
    "tokens = [word_tokenize(texto_final, language=\"spanish\")]\n",
    "          \n",
    "#texto_nltk = nltk.Text(tokens)\n",
    "\n",
    "#texto_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texto_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "frases = sent_tokenize(text = texto_nltk) # Crea tokens de oranciones \n",
    "frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255da5da",
   "metadata": {},
   "source": [
    "### Total de palabras y Palabras únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c131700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_palabras = len(frases)\n",
    "\n",
    "print(f\"Total de palabras en el texto: {total_palabras}\")\n",
    "\n",
    "palabras_diferentes = len(set(frases))\n",
    "\n",
    "print(f\"Total de palabras diferentes en el texto: {palabras_diferentes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eab682",
   "metadata": {},
   "source": [
    "### Riqueza Léxica\n",
    "\n",
    "La riqueza léxica es la relación que existe entre la extensión de un texto y el número de palabras distintas que contiene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "riqueza_lexica = palabras_diferentes / total_palabras\n",
    "\n",
    "print(f\"Riqueza Lexica: {riqueza_lexica}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para calcular la riqueza lexica\n",
    "\n",
    "def riqueza_lexica_fun(frases):\n",
    "    \n",
    "    total_palabras = len(frases)\n",
    "    palabras_diferentes = len(set(frases))\n",
    "    \n",
    "    riqueza_lexica = palabras_diferentes / total_palabras\n",
    "    \n",
    "    return riqueza_lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "riqueza_lexica_fun(frases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aae5b7",
   "metadata": {},
   "source": [
    "### .Text()\n",
    "\n",
    "Transforma un objeto string a un objeto **`Text`** para ser manipulado por **`NLTK`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_nltk = nltk.Text(frases)\n",
    "\n",
    "texto_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(texto_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29f204",
   "metadata": {},
   "source": [
    "### .concordance()\n",
    "\n",
    "Retorna las concordancias de una palabras (todas las veces que aparece en el texto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc224249",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_nltk.concordance(word = \"artículo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ef300",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_nltk.concordance(word = \"constitución\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb23646",
   "metadata": {},
   "source": [
    "### .similar()\n",
    "\n",
    "Encuentra otras palabras que aparecen en el mismo contexto que la palabra especificada, muestra las palabras más similares primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4383dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto_nltk.similar(word = \"constitución\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09216c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar\n",
    "\n",
    "tokens = nltk.word_tokenize(text = frases, language = \"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = sent_tokenize(text = texto) # Crea tokens de oranciones \n",
    "frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palabras = len(tokens)\n",
    "\n",
    "print(f\"Total de palabras en el texto: {total_palabras}\")\n",
    "\n",
    "palabras_diferentes = len(set(tokens))\n",
    "\n",
    "print(f\"Total de palabras diferentes en el texto: {palabras_diferentes}\")\n",
    "\n",
    "print(f\"Riqueza Lexica: {riqueza_lexica_fun(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Text()\n",
    "\n",
    "texto_nltk = nltk.Text(tokens = tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d59408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .concordance()\n",
    "\n",
    "texto_nltk.concordance(word = \"libertad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .similar()\n",
    "\n",
    "texto_nltk.similar(word = \"madre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647ddf0",
   "metadata": {},
   "source": [
    "### .text.ContextIndex() y .similar_words()\n",
    "\n",
    "**`.text.ContextIndex`** y **`.similar_words`** son utilizados para encontrar palabras similares.\n",
    "\n",
    "**Simililares no significa que sean sinónimos, sino que son palabras que van a encontrarse en un contexto similar o tienen similitud con la palabra de busqueda.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548c25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = nltk.text.ContextIndex(tokens = tokens)\n",
    "\n",
    "palabras_similares = idx.similar_words(word = \"madre\")\n",
    "palabras_similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(\"naturaleza madre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si quisieramos encontrar las palabras similares de más de una palabra podemos usar el siguiente bucle:\n",
    "\n",
    "similares = list()\n",
    "\n",
    "for word in nltk.word_tokenize(\"naturaleza madre\"):\n",
    "    similares.append(nltk.text.ContextIndex(tokens).similar_words(word))\n",
    "    \n",
    "pd.DataFrame(data = similares, index = [\"naturaleza\", \"madre\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd53cd7",
   "metadata": {},
   "source": [
    "### .dispersion_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dispersion_plot(words, text, ignore_case=False):\n",
    "    \"\"\"\n",
    "    Crea un gráfico de dispersión para mostrar la aparición de palabras en un texto.\n",
    "    \n",
    "    :param words: Lista de palabras para la dispersión.\n",
    "    :param text: Texto en forma de lista de palabras.\n",
    "    :param ignore_case: Indica si se debe ignorar la distinción entre mayúsculas y minúsculas.\n",
    "    :param title: Título del gráfico.\n",
    "    \"\"\"\n",
    "    if ignore_case:\n",
    "        text = [word.lower() for word in text]\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "    word_positions = {word: [idx for idx, token in enumerate(text) if token == word] for word in words}\n",
    "\n",
    "    plt.figure(figsize=(8, 6))  # Ajustar el tamaño del gráfico\n",
    "    for word, positions in word_positions.items():\n",
    "        plt.scatter(positions, [word] * len(positions), marker='|', label=word)\n",
    "\n",
    "    plt.title(\"Dispersión de Palabras\")\n",
    "    plt.xlabel('Desplazamiento en el Texto')  # Cambiar a eje x\n",
    "    plt.ylabel('Palabras')  # Cambiar a eje y\n",
    "    plt.yticks(rotation=45)  # Rotar las etiquetas del eje y para una mejor visualización\n",
    "    plt.tight_layout()  # Ajustar el diseño del gráfico\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_palabras = [\"las\", \"los\", \"el\", \"ella\", \"ensueño\", \"resplandor\", \"padre\", \"madre\", \"libertad\"] \n",
    "\n",
    "tokens = nltk.word_tokenize(text = texto, language = \"spanish\") \n",
    "texto_nltk = nltk.Text(tokens = tokens) \n",
    "dispersion_plot(lista_palabras, texto_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed917dac",
   "metadata": {},
   "source": [
    "### .FreqDist()\n",
    "\n",
    "Retorna un diccionario donde las llaves son la palabras del texto y el valor son las veces que aparece en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion = nltk.FreqDist(samples = texto_nltk)\n",
    "\n",
    "distribucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12909a",
   "metadata": {},
   "source": [
    "### Hapax\n",
    "\n",
    "Un hapax es una palabra que aparece únicamente una vez en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Text()\n",
    "texto_nltk = nltk.Text(tokens = tokens)\n",
    "\n",
    "# .FreqDist()\n",
    "distribucion = nltk.FreqDist(samples = texto_nltk)\n",
    "\n",
    "# .hapaxes\n",
    "hapaxes = distribucion.hapaxes()\n",
    "\n",
    "for hapax in hapaxes: \n",
    "    print(hapax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hapaxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dispersion_plot(hapaxes[:30], texto_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f3e7d",
   "metadata": {},
   "source": [
    "### STOPWORDS\n",
    "\n",
    "Las **`stopwords`** (palabras funcionales o palabras vacias) son las palabras sin significado como artículos, pronombres, preposiciones, etc. que son filtradas antes o después del procesamiento de datos **NLP**.\n",
    "\n",
    "En ocaciones la eliminacion de stopwords es una tecnica comun en el procesamineto de texto y puede tener varios propositos:\n",
    "\n",
    "- Reducción del ruido: Las stopwords suelen aparecer con mucha frecuencia en el texto y, en muchos casos, no aportan información útil para el análisis. Al eliminar estas palabras, se puede reducir el ruido en el análisis de texto y centrarse en las palabras más relevantes.\n",
    "\n",
    "- Mejora de la eficiencia: Al reducir el número de palabras en un texto, se puede mejorar la eficiencia computacional en tareas de procesamiento de texto, como la  tokenización, el stemming y el cálculo de características en modelos de aprendizaje automático.\n",
    "\n",
    "- Enfoque en las palabras clave: Al eliminar las stopwords, se pueden destacar más fácilmente las palabras clave y los temas importantes en un texto. Esto puede ser útil para tareas como la extracción de información, la clasificación de documentos y el análisis de sentimientos.\n",
    "\n",
    "- Normalización del texto: La eliminación de stopwords puede ayudar a normalizar el texto al reducir las diferencias entre diferentes documentos o fragmentos de texto, lo que facilita la comparación y el análisis.\n",
    "\n",
    "Es importante tener en cuenta que la lista de stopwords puede variar según el contexto y la tarea específica de procesamiento de texto. En algunos casos, puede ser necesario ajustar o personalizar la lista de stopwords según las necesidades del proyecto. Además, en ciertas situaciones, las stopwords pueden contener información útil y su eliminación puede no ser apropiada, por lo que es importante considerar cuidadosamente su uso en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcaf2d3",
   "metadata": {},
   "source": [
    "- Artículos: \"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\".\n",
    "- Pronombres personales: \"yo\", \"tú\", \"él\", \"ella\", \"nosotros\", \"vosotros\", \"ellos\", \"ellas\".\n",
    "- Pronombres demostrativos: \"este\", \"ese\", \"aquel\", \"esta\", \"esa\", \"aquella\".\n",
    "- Pronombres posesivos: \"mi\", \"tu\", \"su\", \"nuestro\", \"vuestro\", \"su\".\n",
    "- Preposiciones: \"a\", \"ante\", \"bajo\", \"con\", \"de\", \"desde\", \"en\", \"entre\", \"hacia\", \"para\", \"por\", \"sin\", \"sobre\", \"tras\".\n",
    "- Conjunciones: \"y\", \"o\", \"pero\", \"porque\", \"si\".\n",
    "- Adverbios de frecuencia: \"siempre\", \"nunca\", \"jamás\", \"a menudo\", \"raramente\", \"tal vez\".\n",
    "- Otros términos comunes: \"ser\", \"estar\", \"tener\", \"hacer\", \"ir\", \"haber\", \"poder\", \"querer\", \"saber\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a344ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "\n",
    "for stopword in stopwords:\n",
    "    print(stopword, end = \" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d09d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text = texto, language = \"spanish\")\n",
    "\n",
    "tokens_limpios = list() \n",
    "\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "for token in tokens: \n",
    "    if token not in stopwords: \n",
    "        tokens_limpios.append(token)\n",
    "        \n",
    "print(f\"Tamaño original: {len(tokens)}\") \n",
    "print(f\"Tamaño despues de stopwords: {len(tokens_limpios)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(nltk.Text(tokens_limpios)).plot(20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos elementos de tamaño 2 o menor:\n",
    "\n",
    "tokens_limpios = list()\n",
    "\n",
    "for token in tokens: \n",
    "    if token not in stopwords: \n",
    "        if len(token) > 2: \n",
    "            tokens_limpios.append(token)\n",
    "            \n",
    "print(len(tokens_limpios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(nltk.Text(tokens_limpios)).plot(20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbac901",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Reducir palabras a su raíz (pueden no tener significado):\n",
    " + Organise, organising, organisation --> organis\n",
    " + intelligence, intelligently --> intelligen\n",
    "\n",
    "Esto ayuda a mejorar la eficiencia y precision del procesamiento del texto.  \n",
    "Estas son algunas razones por la cual hacer stemming es util:\n",
    "1. **Reducción de la dimensionalidad:** Al reducir las palabras a su raíz, se reduce la cantidad de palabras distintas que se manejan en el análisis de texto, lo que puede ayudar a reducir la complejidad computacional y la memoria requerida.\n",
    "\n",
    "2. **Normalización del texto:** El stemming puede ayudar a normalizar el texto al agrupar variantes de una palabra bajo una sola forma base. Esto facilita el análisis al tratar diferentes formas de una palabra como equivalentes.\n",
    "\n",
    "3. **Mejora de la precisión en la recuperación de información:** Al reducir las palabras a su forma base, se pueden agrupar variantes de una palabra en una sola entidad, lo que puede mejorar la precisión al buscar información en un corpus de texto.\n",
    "\n",
    "4. **Preprocesamiento para tareas de minería de texto:** El stemming es a menudo una parte importante del preprocesamiento de texto para tareas de minería de texto, como la clasificación de documentos, la agrupación de documentos y la recuperación de información.\n",
    "\n",
    "Sin embargo, es importante tener en cuenta que el stemming puede no ser perfecto y puede generar raíces que no son palabras reales o pueden generar raíces incorrectas en algunos casos. Esto puede conducir a la pérdida de información semántica o a la introducción de ruido en el análisis de texto. Por esta razón, en algunos casos, se prefieren enfoques más avanzados como la lematización, que tiene en cuenta el contexto de las palabras y puede generar formas base más precisas.\n",
    " \n",
    "Para hacer stemming usamos el algoritmo _**Porter Stemmer:** https://tartarus.org/martin/PorterStemmer/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f189a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97691202",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"hi dear students, how are you doing? we are almost finishing the bootcamp. we hope you are learning and enjoying the course.\"\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos un objeto PorterStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\" \".join([stemmer.stem(word) for word in nltk.word_tokenize(text = texto, language = \"english\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81847143",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "Similar al stemming, pero reduce la palabra a una raíz que sí tiene significado:\n",
    "+ going, goes, gone --> go\n",
    "+ intelligence, intelligently --> intelligent\n",
    "\n",
    "La lematización es útil por varias razones:\n",
    "\n",
    "1. **Normalización del texto**: Ayuda a normalizar las palabras de manera que diferentes formas de la misma palabra se reduzcan a una forma única, lo que facilita el análisis de texto y la extracción de información.\n",
    "\n",
    "2. **Reducción de la dimensionalidad**: Al reducir las palabras a sus lemas, se reduce la cantidad de palabras únicas en el texto, lo que puede mejorar la eficiencia computacional en tareas de PLN como la clasificación de textos, agrupación de documentos, etc.\n",
    "\n",
    "3. **Mejora la precisión**: En muchos casos, la lematización puede ayudar a mejorar la precisión de los modelos de PLN, ya que se enfoca en la esencia semántica de las palabras en lugar de en su forma específica.\n",
    "\n",
    "4. **Mejora la búsqueda de información**: Al lematizar las consultas de búsqueda o los documentos indexados, se puede mejorar la precisión y exhaustividad de los resultados de búsqueda al considerar variantes de palabras.\n",
    "\n",
    "5. **Facilita la interpretación**: Al reducir las palabras a su forma base, se puede facilitar la interpretación del significado del texto, ya que las palabras se presentan en una forma más comprensible y significativa.\n",
    "\n",
    "En resumen, la lematización es una técnica fundamental en el procesamiento del lenguaje natural que ayuda a normalizar y simplificar el texto, lo que facilita su análisis y mejora el rendimiento de las aplicaciones de PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"hi dear students, how are you doing? we are almost finishing the bootcamp. we hope you are learning and enjoying the course.\"\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c286a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos un objeto WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\" \".join([lemmatizer.lemmatize(word, pos = \"v\") for word in nltk.word_tokenize(text = texto, language = \"english\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9f130",
   "metadata": {},
   "source": [
    "### Analisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"The National Guard has been released in Minneapolis to do the job that the Democrat Mayor couldn’t do.\n",
    "           Should have been used 2 days ago & there would not have been damage & Police Headquarters would not have\n",
    "           been taken over & ruined. Great job by the National Guard. No games!\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text = texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    ss = sia.polarity_scores(token)\n",
    "    print(token)\n",
    "    print(ss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('boost')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1ef381ed5e3bd237d5cc3dea30af8018e506ac9a700f35cc38adabbb691cad3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
