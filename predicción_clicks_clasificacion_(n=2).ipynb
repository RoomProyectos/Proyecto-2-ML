{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train, Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clasificadores\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Metricas para Clasificadores\n",
    "from sklearn.metrics import jaccard_score, accuracy_score, recall_score, confusion_matrix, roc_auc_score, f1_score, precision_score\n",
    "\n",
    "# Metodos de validacion\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestro dataframe\n",
    "\n",
    "df = pd.read_csv(\"Data/MeneosCompleto.csv\", delimiter=\";\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_datos(df, output = \"train\"):\n",
    "    #Limpieza de duplicados\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    #Elimina las filas con valores NaN de las columnas que no son Clicks.\n",
    "    lista_columnas = df.columns.to_list()\n",
    "    lista_columnas.remove(\"Clicks\")\n",
    "\n",
    "    for columna in lista_columnas:\n",
    "        lista_indices = df[df[columna].isna() == True].index.tolist()\n",
    "        df = df.drop(index = lista_indices)\n",
    "\n",
    "    # Creamos la columna delay que es el tiempo que transcurre entre que se envía el meneo y se publica\n",
    "    df[\"Delay\"] = df[\"Publicado\"] - df[\"Enviado\"]\n",
    "    \n",
    "    #Eliminación de columnas innecesarias\n",
    "    df = df.drop(columns=[\"Medio\", \"Positivos\", \"Anonimos\", \"Publicado\", \"Enviado\"], axis=1)\n",
    "    \n",
    "    # Separamos nuestro dataframe en uno de train, test y otro de predicción\n",
    "    df_pred = df[df[\"Clicks\"].isna() == True]\n",
    "    df_tt = df[df[\"Clicks\"].isna() == False]\n",
    "\n",
    "    # Eliminación de outliers\n",
    "    df_tt = df_tt[df_tt[\"Meneos\"].between(50, 3000)]\n",
    "    df_tt = df_tt[df_tt[\"Negativos\"] <= 13]\n",
    "    df_tt = df_tt[df_tt[\"Comentarios\"].between(10, 150)]\n",
    "    df_tt = df_tt[df_tt[\"Karma\"].between(230, 700)]\n",
    "    df_tt = df_tt[df_tt[\"Delay\"].between(155, 69761)]\n",
    "\n",
    "    if output == \"train\":\n",
    "        return(df_tt)\n",
    "    else:\n",
    "        df_pred = df_pred.drop(\"Clicks\", axis = 1)\n",
    "        return(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le aplicamos la función de limpieza de datos \n",
    "\n",
    "df_tt = limpieza_datos(df)\n",
    "\n",
    "df_tt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizacion de la columna clicks para decidir en cuántos grupos dividirla\n",
    "\n",
    "px.histogram(df_tt[\"Clicks\"], marginal=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt[\"Clicks\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para dividir la categoría clicks\n",
    "\n",
    "stats_clicks = df_tt[\"Clicks\"].describe()\n",
    "median = stats_clicks[\"50%\"]\n",
    "\n",
    "def clusters_clicks(x):\n",
    "    if x <= median:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le aplicamos la función a la columna \"Clicks\"\n",
    "\n",
    "df_tt[\"Clicks\"] = df_tt[\"Clicks\"].apply(lambda x: clusters_clicks(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASIFICACION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificadores = [KNeighborsClassifier(),\n",
    "                  RadiusNeighborsClassifier(radius=0.5),\n",
    "                  NearestCentroid(),\n",
    "                  LogisticRegression(),\n",
    "                  GaussianNB(),\n",
    "                  DecisionTreeClassifier(),\n",
    "                  RandomForestClassifier(),\n",
    "                  SVC(),\n",
    "                  AdaBoostClassifier(algorithm = \"SAMME\"),\n",
    "                  GradientBoostingClassifier()]\n",
    "\n",
    "X = df_tt.iloc[:, 2:].drop(\"Clicks\", axis = 1)\n",
    "y = np.array(df_tt[\"Clicks\"])\n",
    "\n",
    "x_scaler_class = MinMaxScaler()\n",
    "X = x_scaler_class.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "datos_clf = list()\n",
    "\n",
    "for clf in clasificadores:\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    yhat = clf.predict(X_test)\n",
    "    \n",
    "    jac = jaccard_score(y_test, yhat)\n",
    "    acc = accuracy_score(y_test, yhat)\n",
    "    rec = recall_score(y_test, yhat)\n",
    "    cmx = confusion_matrix(y_test, yhat)\n",
    "    roc = roc_auc_score(y_test, yhat)\n",
    "    f1_ = f1_score(y_test, yhat)\n",
    "    pre = precision_score(y_test, yhat)\n",
    "    \n",
    "    datos_clf.append([str(clf), clf, jac, acc, rec, cmx, roc, f1_, pre])\n",
    "    \n",
    "df_metrics = pd.DataFrame(data = datos_clf, columns = [\"name\", \"clf\", \"jaccard\", \"accuracy\", \"recall\",\n",
    "                                                       \"confusion_matrix\", \"roc_auc\", \"f1_score\", \"precision\"])\n",
    "\n",
    "df_metrics.sort_values(\"roc_auc\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "\n",
    "X = df_tt.iloc[:, 2:].drop(\"Clicks\", axis = 1)\n",
    "y = np.array(df_tt[\"Clicks\"])\n",
    "\n",
    "x_scaler_class = MinMaxScaler()\n",
    "X = x_scaler_class.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "# Modelo\n",
    "GradientBoosting = GradientBoostingClassifier()\n",
    "\n",
    "# Parametros a iterar\n",
    "parametros = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.2],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]    \n",
    "}\n",
    "\n",
    "# Metricas\n",
    "scorers = [\"accuracy\", \"jaccard\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "\n",
    "# GridSearchCV\n",
    "grid_solver = GridSearchCV(estimator  = GradientBoosting,\n",
    "                           param_grid = parametros,\n",
    "                           scoring    = scorers,\n",
    "                           cv         = 2,\n",
    "                           refit      = \"roc_auc\",\n",
    "                           n_jobs     = -1,\n",
    "                           verbose    = 3)\n",
    "\n",
    "# Resultados\n",
    "model_result = grid_solver.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo entrenado\n",
    "\n",
    "best_GradientBoosting = model_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = limpieza_datos(df, output=\"pred\")\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "\n",
    "X = df_pred.iloc[:, 2:]\n",
    "\n",
    "x_scaler_class = MinMaxScaler()\n",
    "X = x_scaler_class.fit_transform(X)\n",
    "\n",
    "# Predicción\n",
    "y_hat_final = best_GradientBoosting.predict(X)\n",
    "\n",
    "# Añadimos predicción al df\n",
    "df_pred[\"Clicks_clase\"] = y_hat_final    \n",
    "df_pred[\"Clicks\"] = df_pred[\"Clicks_clase\"].apply(lambda x: f\"{int(median)} clicks o menos\" if 0 else f\"Más de {int(median)} clicks\")\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
