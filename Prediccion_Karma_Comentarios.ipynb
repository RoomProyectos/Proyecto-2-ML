{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Prepocesamiento\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train-Test, Validación\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "# Regresores\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Metricas para regresiones\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "\n",
    "# Visualizaciones\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga de librerías adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    %pip install spacy\n",
    "    !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del dataset completo de comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = pd.read_csv(\"Data/100178_Comentarios.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción de la feature \"texto\" y el target \"karma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_completo[[\"texto\", \"karma\"]]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de valores nulos y conversión de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total de nulos por columna\\n{df.isna().sum()}\")\n",
    "df = df.dropna()\n",
    "df[\"karma\"] = df[\"karma\"].astype(\"int32\")\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de funciónes auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar de las stopwords en español de nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
    "\n",
    "# Cargar del modelo de lenguaje para español de Spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función para la limpieza de comentarios\n",
    "def limpieza_texto(texto: str):\n",
    "    '''\n",
    "    Función de limpieza de texto que elimina números, signos de puntuación y caractereres no alfanuméricos.\\n\n",
    "    Elimina stopwords, lematiza la entrada y devuelve una cadena de texto con todas las palabras de más de 2 letras. \\n\n",
    "    Necesita definición previa de la lista de \"stopwords\" y la inicialización del modelo de lenguaje de Spacy en la variable \"nlp\"\n",
    "    '''\n",
    "    texto = re.sub(r'[^a-zA-Z0-9\\sáéíóúñçÁÉÍÓÚäëïöüàèìòù]', ' ', texto)\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "\n",
    "    # Lematización\n",
    "    comentario_nlp = nlp(texto)\n",
    "    lemmas = [token.lemma_ for token in comentario_nlp]\n",
    "    comentario_stopwords = ' '.join(lemmas)\n",
    "    \n",
    "    # Elimnación de Stopwords y palabras de menos de 3 letras\n",
    "    tokens_limpios = [palabra for palabra in comentario_stopwords.split(\" \") if palabra.lower() not in stopwords and len(palabra) > 2]\n",
    "    \n",
    "    return ' '.join(tokens_limpios)\n",
    "\n",
    "# Función para eliminar hapaxes\n",
    "def eliminar_palabras(texto: str, lista: list[str]):\n",
    "    '''\n",
    "    Elimina de un texto dado las palabras contenidas en la lista de palabras provista\n",
    "    '''    \n",
    "    texto_limpio = ' '.join([palabra for palabra in texto.split(\" \") if palabra not in lista])\n",
    "    return texto_limpio\n",
    "\n",
    "def DistribucionPalabras_RiquezaLexica(datos: str):\n",
    "    '''\n",
    "    Función para mostrar la riqueza léxica de un texto.\\n\n",
    "    Retorna la distribución de palabras (nltk.FreqDist).\n",
    "    \n",
    "    '''\n",
    "    # Tokenización del texto y generación de la distribución\n",
    "    tokens = nltk.word_tokenize(text = datos, language = \"spanish\") \n",
    "    texto_nltk = nltk.Text(tokens = tokens)\n",
    "    distribucion = nltk.FreqDist(samples = texto_nltk)\n",
    "\n",
    "    # Obtención de la riqueza Léxica\n",
    "    total_palabras = len(tokens)\n",
    "    palabras_diferentes = len(set(tokens))\n",
    "    riqueza_lexica = round((palabras_diferentes / total_palabras) * 100, 2)\n",
    "    \n",
    "    # Obtención de hapaxes\n",
    "    hapaxes = distribucion.hapaxes()\n",
    "\n",
    "    print(f\"Total de palabras: {total_palabras}\")\n",
    "    print(f\"Palabras diferentes: {palabras_diferentes}\")\n",
    "    print(f\"Riqueza Lexica: {riqueza_lexica}% palabras distintas\")\n",
    "    print (f\"Número total de hapaxes: {len(hapaxes)}\")\n",
    "\n",
    "    return distribucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del texto\n",
    "Se eliminan también los posibles nulos resultantes tras la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texto\"] = df[\"texto\"].apply(limpieza_texto)\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exportacion Intermedia de Datos y obtención del texto completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"Data/100K_Comentarios_TrasLimpieza.csv\", sep=\";\", encoding=\"utf-8\",  index=False)\n",
    "try:\n",
    "    with open(\"Data/Texto_Comentarios_100K.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        texto_completo = file.read()\n",
    "except:\n",
    "    texto_completo = \"\"\n",
    "    try:\n",
    "        os.mkdir(\"Data\")\n",
    "    except:\n",
    "        pass\n",
    "    with open(\"Data/Texto_Comentarios_100K.txt\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        for fila in df[\"texto\"].values:\n",
    "            file.write(fila + \" \")\n",
    "            texto_completo += fila + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación y graficado de la Distribución de palabras y muestra de la riqueza léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion = DistribucionPalabras_RiquezaLexica(texto_completo)\n",
    "distribucion.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de hapaxes y aplicación de la función de eliminación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapaxes = distribucion.hapaxes()\n",
    "df[\"texto\"] = df[\"texto\"].apply(lambda x: eliminar_palabras(x, hapaxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exportación intermedia de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"Data/100KComentarios_Procesados.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "except:\n",
    "    try:\n",
    "        os.mkdir(\"Data\")\n",
    "    except:\n",
    "        pass\n",
    "    df.to_csv(\"Data/100KComentarios_Procesados.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "try:\n",
    "    with open(\"Data/Texto_Comentarios_100K_Procesados.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        texto_completo_procesado = file.read()\n",
    "except:\n",
    "    texto_completo_procesado = \"\"\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(\"Data\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    with open(\"Data/Texto_Comentarios_100K_Procesados.txt\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        for fila in df[\"texto\"].values:\n",
    "            file.write(fila + \" \")\n",
    "            texto_completo_procesado += fila + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución tras la eliminación de hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion = DistribucionPalabras_RiquezaLexica(texto_completo_procesado)\n",
    "distribucion.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion.hapaxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de la variable objetivo \"karma\"\n",
    "Se observa que la variable toma valores negativos y sufre un desbalanceo de la distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df[\"karma\"], marginal=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1º - Escalado para eliminar valores negativos\n",
    "\n",
    "2º - Normalización con la función logaritmica. Se añade desplazamiento para evitar el logaritmo de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karma_scaler = MinMaxScaler()\n",
    "karma_escalado = karma_scaler.fit_transform(df[\"karma\"].values.reshape(-1,1))\n",
    "karma_normalizado = np.log(karma_escalado + 1)\n",
    "px.histogram(karma_normalizado, marginal=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los cuantiles, el rango intercuartil y los bigotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karma_Q1 = pd.DataFrame(karma_normalizado)[0].describe()[\"25%\"]\n",
    "karma_Q3 = pd.DataFrame(karma_normalizado)[0].describe()[\"75%\"]\n",
    "\n",
    "karma_RIC = karma_Q3 - karma_Q1\n",
    "\n",
    "bigote_superior = karma_Q3 + 1.5*karma_RIC\n",
    "bigote_inferior = karma_Q1 - 1.5*karma_RIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación inversa (exponenciación) y desescalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigote_superior = int(round(karma_scaler.inverse_transform((np.exp(bigote_superior) - 1).reshape(1,-1))[0][0],0))\n",
    "bigote_inferior = int(round(karma_scaler.inverse_transform((np.exp(bigote_inferior) - 1).reshape(1,-1))[0][0],0))\n",
    "print(f\"Bigote superior: {bigote_superior}\\nBigote inferior: {bigote_inferior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de Outliers de acuerdo a los bigotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"karma\"] >= bigote_inferior) & (df[\"karma\"] <= bigote_superior)]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conteo y Vectorizado de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos un objeto CountVecrtorizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Entrenamos el modelo y transformamos los datos.\n",
    "bag = count_vectorizer.fit_transform(df[\"texto\"])\n",
    "\n",
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación de la matriz de palabras por frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos un objeto Tfidf\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Entrenamos el Tfidf y transformamos la variable bag\n",
    "bag_tfidf = tfidf.fit_transform(bag)\n",
    "bag_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de los grupos de train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_tfidf, # X\n",
    "                                                    df[\"karma\"].values, # y\n",
    "                                                    test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape},  y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado del target para eliminar valores negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = y_scaler.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación logarítmica para acercarse a una distribución normal (gaussiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train + 1)\n",
    "y_test = np.log(y_test + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listado inicial de modelos\n",
    "\n",
    "Se descartan RadiusNeighborsRegressor, RandomForestRegressor y SVR por su coste computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = [LinearRegression(),\n",
    "            KNeighborsRegressor(),            \n",
    "            DecisionTreeRegressor(),            \n",
    "            AdaBoostRegressor(),\n",
    "            GradientBoostingRegressor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento y predicción con todos los modelos y generación de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_modelos = list()\n",
    "\n",
    "for model in modelos:\n",
    "    print (f\"    #### Modelo: {str(model)} ####\")\n",
    "    model.fit(X_train, y_train.reshape(-1,))\n",
    "    yhat = model.predict(X_test)\n",
    "\n",
    "    y_test_inv = np.exp(y_test) - 1\n",
    "    yhat_inv = np.exp(yhat) - 1\n",
    "\n",
    "    y_test_inv = y_scaler.inverse_transform(y_test_inv)\n",
    "    yhat_inv = y_scaler.inverse_transform(yhat_inv.reshape(-1,1))\n",
    "\n",
    "    #Metricas\n",
    "    mae = mean_absolute_error(y_test_inv, yhat_inv)\n",
    "    mse = mean_squared_error(y_test_inv, yhat_inv)\n",
    "    rmse = root_mean_squared_error(y_test_inv, yhat_inv)\n",
    "    r2 = r2_score(y_test_inv, yhat_inv)\n",
    "    \n",
    "    datos_modelos.append([str(model).strip(\"()\"), model, mae, mse, rmse, r2])\n",
    "    print(f\"-MAE: {mae}  -MSE: {mse}  -RMSE: {rmse}  -R2: {r2}\")\n",
    "\n",
    "df_modelo = pd.DataFrame(data = datos_modelos, columns = [\"name\", \"model\", \"mae\", \"mse\", \"rmse\", \"r2\"])\n",
    "df_modelo.sort_values(\"mae\", ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Modelo: LinearRegression() ####\n",
    "-MAE: 17.35527958409468  -MSE: 3524.3815902220676  -RMSE: 59.36650225692994  -R2: -22.48181281609597\n",
    "\n",
    "#### Modelo: KNeighborsRegressor() ####\n",
    "-MAE: 9.610798384177029  -MSE: 171.61590924481843  -RMSE: 13.100225541753792  -R2: -0.14342120851250484\n",
    "\n",
    "#### Modelo: DecisionTreeRegressor() ####\n",
    "-MAE: 11.707938027584852  -MSE: 273.7842466561918  -RMSE: 16.546427005737275  -R2: -0.8241357433635506\n",
    "\n",
    "#### Modelo: AdaBoostRegressor() ####\n",
    "-MAE: 10.10279784649557  -MSE: 154.21218886035678  -RMSE: 12.41822003591323  -R2: -0.02746585750698216\n",
    "\n",
    "#### Modelo: GradientBoostingRegressor() ####\n",
    "-MAE: 9.292092233264496  -MSE: 149.94135404952226  -RMSE: 12.245054268949659  -R2: 0.0009893313053519481\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinamos el modelo con mejores métricas - GradientBoostingRegressor\n",
    "Imprimimos las métricas como referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "print (f\"#### Modelo: {str(model).strip('()')} ####\")\n",
    "model.fit(X_train, y_train.reshape(-1,))\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "y_test_inv = np.exp(y_test) - 1\n",
    "yhat_inv = np.exp(yhat) - 1\n",
    "\n",
    "y_test_inv = y_scaler.inverse_transform(y_test_inv)\n",
    "yhat_inv = y_scaler.inverse_transform(yhat_inv.reshape(-1,1))\n",
    "\n",
    "#Metricas\n",
    "mae = mean_absolute_error(y_test_inv, yhat_inv)\n",
    "mse = mean_squared_error(y_test_inv, yhat_inv)\n",
    "rmse = root_mean_squared_error(y_test_inv, yhat_inv)\n",
    "r2 = r2_score(y_test_inv, yhat_inv)\n",
    "\n",
    "print(f\"MAE: {mae}\\nMSE: {mse}\\nRMSE: {rmse}\\nR2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Procedemos a la validación mediante KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos realizar 5 folds.\n",
    "Validacion_kfold = KFold(n_splits = 5)\n",
    "\n",
    "# Creación de listas para los resultados generales y parciales\n",
    "lista_targets = list()\n",
    "lista_predicciones = list()\n",
    "lista_mae = list()\n",
    "lista_rmse = list()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in Validacion_kfold.split(df):\n",
    "    i += 1\n",
    "    print (f\"#### Fold {i} ####\")\n",
    "    \n",
    "    # Creación de los conjuntos de Train y Test\n",
    "    X_train, X_test = bag_tfidf[train_index], bag_tfidf[test_index]\n",
    "    y_train, y_test = df[\"karma\"][train_index].values, df[\"karma\"][test_index].values\n",
    "\n",
    "    # Normalización de la columna objetivo en ambos conjuntos\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "    y_test = y_scaler.transform(y_test.reshape(-1,1))\n",
    "    y_train = np.log(y_train + 1)\n",
    "    y_test = np.log(y_test + 1)\n",
    "    \n",
    "    # Creación y entrenamiento del Modelo    \n",
    "    GradientBoosting = GradientBoostingRegressor()\n",
    "    GradientBoosting.fit(X_train, y_train.reshape(-1,))\n",
    "\n",
    "    # Prediccion\n",
    "    yhat = GradientBoosting.predict(X_test)\n",
    "\n",
    "    y_test_inv = np.exp(y_test) - 1\n",
    "    yhat_inv = np.exp(yhat) - 1\n",
    "\n",
    "    y_test_inv = y_scaler.inverse_transform(y_test_inv)\n",
    "    yhat_inv = y_scaler.inverse_transform(yhat_inv.reshape(-1,1))\n",
    "\n",
    "    # Metricas\n",
    "    mae = mean_absolute_error(y_test_inv, yhat_inv)    \n",
    "    rmse = root_mean_squared_error(y_test_inv, yhat_inv)\n",
    "    print(f\"MAE: {mae}\\nRMSE: {rmse}\\n\")\n",
    "\n",
    "    # Se añaden las métricas a listas\n",
    "    lista_targets.extend(y_test_inv)\n",
    "    lista_predicciones.extend(yhat_inv)\n",
    "    lista_mae.append(mae)\n",
    "    lista_rmse.append(rmse)\n",
    "\n",
    "# Se calcula el desempeño general y se imprimen los rangos de las métricas observadas\n",
    "print(f\"GENERAL MAE:\\t {mean_absolute_error(lista_targets, lista_predicciones)}\")\n",
    "print(f\"GENERAL RMSE:\\t {root_mean_squared_error(lista_targets, lista_predicciones)}\\n\")\n",
    "\n",
    "print(f\"Min de MAE: {np.array(lista_mae).min()}\")\n",
    "print(f\"Media de MAE: {np.array(lista_mae).mean()}\")\n",
    "print(f\"Max de MAE: {np.array(lista_mae).max()}\\n\")\n",
    "\n",
    "print(f\"Min de RMSE: {np.array(lista_rmse).min()}\")\n",
    "print(f\"Media de RMSE: {np.array(lista_rmse).mean()}\")\n",
    "print(f\"Max de RMSE: {np.array(lista_rmse).max()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas resultan muy similares en todos los folds.\n",
    "\n",
    "Se presume que el modelo es consistente independientemente de lo datos de entrenamiento/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejora de parámetros mediante GridSearchCV\n",
    "\n",
    "Se ejecuta el GridSearchCV con los parámetros establecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y = np.log(y_scaler.fit_transform(df[\"karma\"].values.reshape(-1,1)) + 1 )\n",
    "\n",
    "# Modelo\n",
    "GradientBoosting = GradientBoostingRegressor()\n",
    "\n",
    "# Parametros a iterar\n",
    "parametros = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.2],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]    \n",
    "}\n",
    "\n",
    "# Metricas\n",
    "scorers = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\"]\n",
    "\n",
    "# GridSearchCV\n",
    "grid_solver = GridSearchCV(estimator  = GradientBoosting,\n",
    "                           param_grid = parametros,\n",
    "                           scoring    = scorers,\n",
    "                           cv         = 2,\n",
    "                           refit      = \"neg_mean_absolute_error\",\n",
    "                           n_jobs     = -1,\n",
    "                           verbose    = 2)\n",
    "\n",
    "# Resultados\n",
    "model_result = grid_solver.fit(bag_tfidf, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "223m 18.5s\n",
    "Fitting 2 folds for each of 72 candidates, totalling 144 fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se imprime el resultado de la métrica a maximizar y los mejores parámetros\n",
    "\n",
    "Al tratarse de una métrica de error, la cual se trata de minimizar típicamente, el GridSearchCV realiza la multiplicación por -1 a la hora de calcularla y por tanto poder maximizar la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_real = np.exp(model_result.best_score_) - 1\n",
    "resultado_real = y_scaler.inverse_transform(resultado_real.reshape(1,-1))\n",
    "print(f\"Media de MAE: {model_result.cv_results_[\"mean_test_neg_mean_absolute_error\"].mean()}\")\n",
    "print(f\"Media de MSE: {model_result.cv_results_[\"mean_test_neg_mean_squared_error\"].mean()}\")\n",
    "\n",
    "print(f\"Mejor resultado tras el escalado: {resultado_real}\")\n",
    "print(f\"Mejores parámetros:\\n{model_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se establecen los parámetros obtenidos y se entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_final = GradientBoostingRegressor(learning_rate=0.05,\n",
    "                                         max_depth=4,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         min_samples_split=10,\n",
    "                                         n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_tfidf, # X\n",
    "                                                    df[\"karma\"].values, # y\n",
    "                                                    test_size = 0.2, random_state = 42)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = y_scaler.transform(y_test.reshape(-1,1))\n",
    "\n",
    "y_train = np.log(y_train + 1)\n",
    "y_test = np.log(y_test + 1)\n",
    "\n",
    "print (f\"#### Modelo: {str(modelo_final)} ####\")\n",
    "modelo_final.fit(X_train, y_train.reshape(-1,))\n",
    "yhat = modelo_final.predict(X_test)\n",
    "\n",
    "y_test_inv = np.exp(y_test) - 1\n",
    "yhat_inv = np.exp(yhat) - 1\n",
    "\n",
    "y_test_inv = y_scaler.inverse_transform(y_test_inv)\n",
    "yhat_inv = y_scaler.inverse_transform(yhat_inv.reshape(-1,1))\n",
    "\n",
    "#Metricas\n",
    "mae = mean_absolute_error(y_test_inv, yhat_inv)\n",
    "mse = mean_squared_error(y_test_inv, yhat_inv)\n",
    "rmse = root_mean_squared_error(y_test_inv, yhat_inv)\n",
    "r2 = r2_score(y_test_inv, yhat_inv)\n",
    "\n",
    "print(f\"MAE: {mae}\\nMSE: {mse}\\nRMSE: {rmse}\\nR2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de textos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_prueba = [\"Científicos de la Universidad de Australia Occidental y Kelpie Geosciences en el Reino Unido filmaron en vídeo un raro calamar de aguas profundas a una profundidad de más de un kilómetro bajo el agua. El calamar anzuelo de aguas profundas es uno de los calamares de aguas profundas más grandes y es famoso por tener dos fotóforos muy grandes en el extremo de dos de sus brazos, que producen brillantes destellos bioluminiscentes para asustar y desorientar a sus presas cuando caza. Estos son los fotóforos más grandes conocidos en el mundo natural.\",\n",
    "                 \"Aguasprofundas, calamar, ningún chiste sobre un desuellamentes o Illithid en los primeros 10 comentarios.... Algo falla aquí. ¿Ha subido la media de edad?\",\n",
    "                 \"Alaa es una niña. Vive (o más bien sobrevive) en la franja de Gaza. Junto con su familia (los que aún viven) ha sido desplazada a Rafah. Sí, esa ciudad a donde Israel obligó a refugiarse a los que …\",\n",
    "                 \"#3 Pero si para la ultraizquierda todo el que no sea como ellos, es 'fascista'. Aquí en España los desquiciados votantes socialcomunistas llamaban 'trifachito' a PP, Cs y VOX. Los 'demócratas', la izquierda que expresa públicamente que quiere prohibir todo lo que no sea socialcomunista.\",\n",
    "                 \"Estaremos todos de acuerdo en una persona que razona así no puede estar en contacto con niños,si?\"]\n",
    "\n",
    "Serie_pruebas = pd.DataFrame(textos_prueba, columns=[\"texto\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se aplica la limpieza del texto, se transforma con los objetos previamente entrenados y se realiza la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Serie_pruebas[\"texto\"] = Serie_pruebas[\"texto\"].apply(limpieza_texto)\n",
    "bag_prueba = count_vectorizer.transform(Serie_pruebas[\"texto\"])\n",
    "bag_tfidf_prueba = tfidf.transform(bag_prueba)\n",
    "resultado_prueba = modelo_final.predict(bag_tfidf_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se realiza la transformación inversa y el desescalado y se añaden los resultados al los textos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_prueba_inv = np.exp(resultado_prueba) - 1\n",
    "resultado_prueba_inv = y_scaler.inverse_transform(resultado_prueba_inv.reshape(-1,1))\n",
    "Serie_pruebas[\"karma\"] = resultado_prueba_inv\n",
    "Serie_pruebas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
